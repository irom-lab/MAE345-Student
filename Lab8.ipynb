{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: (Single-Layer) Neural Networks and Gradient Descent (50 Pts)\n",
    "\n",
    "In this lab, we are going to implement gradient descent to train a simple (single-layer) neural network. We will use the Jax software library for doing this:\n",
    "https://github.com/google/jax\n",
    "\n",
    "Before we begin, we will first install Jax. To do this, run the following commands in your terminal:\n",
    "\n",
    "`pip3 install --upgrade pip`\n",
    "\n",
    "`pip install --user --upgrade jax jaxlib`\n",
    "\n",
    "The main functionality that Jax offers is the ability to automatically compute gradients (so you don't have to do this by hand!). You can write mathematical functions using Numpy syntax and Jax will allow  you to compute gradients. As an example, take a look at the block of code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "import jax.numpy as np\n",
    "\n",
    "# Define a function using numpy operations \n",
    "def my_function(w,x,y):\n",
    "    return np.power(w,2)*x + y\n",
    "\n",
    "# Gradient function (partial derivative of my_function w.r.t. the first argument, i.e., w)\n",
    "gradient_fun = grad(my_function) \n",
    "\n",
    "# Evaluate the gradient (partial derivative) of my_function with respect to w at w=w0, x=x0, y=y0\n",
    "w0 = 0.5\n",
    "x0 = 0.2\n",
    "y0 = 0.3\n",
    "print(gradient_fun(w0, x0, y0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually verify that `gradient_fun(x0, x0, y0)` evaluates the gradient of `my_function` w.r.t. the first argument of `my_function` (which is `w`) at `w0, x0, y0`. The beauty of Jax is that `my_function` can be extremely complicated; manually computing the gradient might be very annoying, but Jax saves you the trouble. \n",
    "\n",
    "Notice also that we are not actually using numpy above. We are using jax.numpy as if it were numpy. Jax has overloaded a large number of numpy operations. As long as `my_function` uses these basic numpy operations, Jax will allow you to compute gradients. You can take a look at the Jax documentation for all the numpy functions Jax has overloaded (but we won't need anything fancy for this assignment -- you can just pretend like jax.numpy is numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train a single-layer neural network using gradient descent. Your task is to fill in the portions marked \"TODO\". \n",
    "\n",
    "First, let's define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jax import grad, jit, ops\n",
    "import jax.numpy as np\n",
    "\n",
    "# Sigmoid nonlinearity (i.e., activation)\n",
    "def sigmoid(z):\n",
    "    return 0.5 * (np.tanh(z / 2.) + 1)\n",
    "\n",
    "######## TODO: Fill in these functions ################################\n",
    "\n",
    "# Single-layer neural network (with no bias) [10 pts]\n",
    "def neural_network_prediction(weights, x):\n",
    "    '''This function should implement a single-layer neural network. We will\n",
    "    ignore the bias term here to keep things simple. This function\n",
    "    should take in an input (x) and output sigmoid(w'*x). Recall that\n",
    "    matrix multiplication in numpy can be done using np.dot().'''\n",
    "    return \n",
    "\n",
    "# Loss function: Binary Cross Entropy [5 pts]\n",
    "def loss_function(y_pred, y_true):\n",
    "    '''Loss function: this function takes in a predicted label (scalar)\n",
    "    and a true label (scalar) and outputs a scalar that quantifies how\n",
    "    good the prediction is. Implement the binary cross entropy loss\n",
    "    discussed in Lecture 20.'''\n",
    "    return \n",
    "\n",
    "# Training loss [20 pts]\n",
    "def training_loss(weights, inputs, labels):\n",
    "    '''Compute the total training loss here (i.e., the loss summed\n",
    "    over all the inputs in the dataset)'''   \n",
    "    return \n",
    "\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define some training data (inputs (x) and labels (y)). Each column of `inputs` corresponds to a datapoint (i.e., a single x). The array `labels` contains the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY ############################################\n",
    "# Build a toy dataset\n",
    "inputs = np.array([[1.0,  0.0, 0.25, -1.0,  0.0, -0.25],\n",
    "                   [0.0,  1.0, 0.3,   0.0, -1.0, -0.4]])\n",
    "\n",
    "labels = np.array([1, 1, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will optimize our weights to minimize the training loss using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY #############################################\n",
    "# Define a function that returns gradients of the training loss.\n",
    "# This is identical to the simple gradient example we considered at the\n",
    "# very beginning of this notebook. The \"jit\" command does \"just-in-time\n",
    "# compilation\". Don't worry too much about what jit is. This helps speed\n",
    "# up the code. \n",
    "training_gradient_fun = jit(grad(training_loss))\n",
    "########################################################################\n",
    "\n",
    "# Optimize weights using gradient descent\n",
    "weights = np.ones(2) # Initialize the weights to the all-ones vector (this doesn't really matter. Random\n",
    "# initializations would be ok too.)\n",
    "# Print the training loss with the initial weights\n",
    "print(\"Initial loss:\", training_loss(weights, inputs, labels))\n",
    "\n",
    "########### TODO: Fill in code to do gradient descent [15 pts] ########\n",
    "step_size = # Choose a step size\n",
    "# Take gradient steps\n",
    "for i in range(100):\n",
    "    # Write gradient descent step here\n",
    "\n",
    "\n",
    "    \n",
    "# Print the training loss with the optimized weights    \n",
    "print(\"Trained loss:\", training_loss(weights, inputs, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the block below visualizes the outputs of the trained neural network. Note that the true labels are either 1 (plus) or 0 (dot) and the output of your single-layer neural network will also be between 0 and 1. The contour plot below shows level-sets of the neural network's predictions. The level set corresponding to 0.5 is the \"decision boundary\". If trained correctly, the 0.5-level set should separate the points with label equals to 1 from the points with labels equal to 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ DO NOT MODIFY #############################################\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as npp\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(inputs[0,0:3], inputs[1,0:3], c='r',marker='+')\n",
    "plt.scatter(inputs[0,3:6], inputs[1,3:6], c='b',marker='.')\n",
    "\n",
    "delta = 0.2\n",
    "xs = np.arange(-1.5, 1.5, delta)\n",
    "ys = np.arange(-1.5, 1.5, delta)\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "\n",
    "Z = npp.zeros(np.shape(X))\n",
    "for i in range(0,len(xs)):\n",
    "    for j in range(0,len(xs)):\n",
    "        xi = np.transpose(np.array([X[i,j], Y[i,j]]))\n",
    "        Z[i,j] = neural_network_prediction(weights, xi)\n",
    "\n",
    "\n",
    "CS = ax.contour(X, Y, Z) \n",
    "\n",
    "ax.clabel(CS, inline=1, fontsize=10)\n",
    "ax.set_title('Neural network predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "As usual, you can submit your completed notebook [here](https://www.dropbox.com/request/L7DAefVsTWLLd1sBniQJ)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
